{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead1a743",
   "metadata": {},
   "source": [
    "## Web Scraper\n",
    "We are scrapping links from direct atheletics (https://www.directathletics.com/rankings.html) to scrap data from the main athletic database (https://www.tfrrs.org/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693b0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\python\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in d:\\python\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in d:\\python\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: matplotlib in d:\\python\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: seaborn in d:\\python\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\python\\lib\\site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\python\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joeyt\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\python\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\python\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\python\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joeyt\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\python\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\python\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\python\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joeyt\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'd:\\Python\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Download all necessary libraries\n",
    "%pip install requests beautifulsoup4 pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d4f0b",
   "metadata": {},
   "source": [
    "## Scrapping Team Links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1142ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import random\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "# URL Link for NCCA Teams\n",
    "ncaa_url = 'https://www.directathletics.com/leagues/track/48.html'\n",
    "naia_url = 'https://www.directathletics.com/leagues/track/53.html'\n",
    "tfrrs_url = 'https://www.tfrrs.org'\n",
    "\n",
    "# User_Agents to avoid web scraping detection\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0',\n",
    "]\n",
    "\n",
    "# Get all the team links from the page\n",
    "def get_team_links(html_content):\n",
    "    \"\"\"Parses HTML content to extract team links\n",
    "    \n",
    "    Parameters:\n",
    "    - html (str): HTML content of the web page\n",
    "    \n",
    "    Returns:\n",
    "    - List[String]: A list of strings where each string is a team link \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    team_links = []\n",
    "    \n",
    "    links = soup.findAll('a', {'class': 'pLinks'})\n",
    "    \n",
    "    for team in links:\n",
    "        if 'teams' in team['href']:\n",
    "            team_links.append(team['href'])\n",
    "    \n",
    "    return team_links\n",
    "\n",
    "# Get all the atheltes links \n",
    "def get_all_atheltes_links(urls):\n",
    "    \"\"\"Extracts all athlete links from a list of team URLs\n",
    "    \n",
    "    Parameters:\n",
    "    - urls (List[String]): A list of strings where each string is a team URL\n",
    "    \n",
    "    Returns:\n",
    "    - List[String]: A list of strings where each string is an athlete link \"\"\"\n",
    "    \n",
    "    athlete_links = []\n",
    "    \n",
    "    for url in urls:\n",
    "        html = make_request(url)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        link = soup.find('script').contents[0].split()[2][1:-2]\n",
    "        athlete_links.append(get_athelte_links(link))\n",
    "    \n",
    "    return athlete_links\n",
    "\n",
    "# Get all the athlete links from one team page\n",
    "def get_athelte_links(html):\n",
    "    \"\"\"Parses HTML content to extract athlete links\n",
    "    \n",
    "    Parameters:\n",
    "    - html (str): HTML content of the web page\n",
    "    \n",
    "    Returns:\n",
    "    - List[String]: A list of strings where each string is an athlete link \"\"\"\n",
    "    \n",
    "    # Make Request to the URL\n",
    "    html_content = make_request(html)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Get if the team is male or female \n",
    "    gender = 'M' if 'm' in html.split('_') else 'F'\n",
    "    \n",
    "    athlete_links = []\n",
    "    athlete_info = []\n",
    "    \n",
    "    links = soup.findAll('a', {'data-turbo-frame': '_top', 'data-turbo': 'false'})\n",
    "    \n",
    "    individual_links = links.findAll('a', {'class':'btn'})\n",
    "    \n",
    "    print(individual_links)\n",
    "    \n",
    "    for athlete in individual_links:\n",
    "        if 'athletes' in athlete['href']:\n",
    "            athlete_links.append(tfrrs_url + athlete['href'])\n",
    "            \n",
    "    # print(athlete_links)\n",
    "            \n",
    "    # for athlete in athlete_links:\n",
    "    #     athlete_info.append(get_athelte_info(athlete, gender))\n",
    "    \n",
    "    return athlete_info\n",
    "\n",
    "# Function to rotate user-agents\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# Function to make a request to a URL\n",
    "def make_request(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    \n",
    "     # Delay Each Call\n",
    "    random_delay = randint(50, 175)/100\n",
    "    sleep(random_delay)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# Get all the athlete information \n",
    "def get_athelte_info(html, gender):\n",
    "    \"\"\"Parses HTML content to extract athlete info\n",
    "    \n",
    "    Parameters:\n",
    "    - html (str): HTML of the web page\n",
    "    \n",
    "    Returns:\n",
    "    - Dict[]: A list of strings where each string is an athlete link \"\"\"\n",
    "    # Make Request to the URL\n",
    "    html_content = make_request(html)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    team = html.split('/')[-2]\n",
    "    athlete_name = html.split('/')[-1].replace('_', ' ')[:-5]\n",
    "    performances = []\n",
    "    \n",
    "    event_elements = soup.find_all(class_='table table-hover >')\n",
    "    \n",
    "    for event in event_elements:\n",
    "        event_header = event.find('th')\n",
    "        # event_name = event_header.find('a').text.strip()\n",
    "        event_date = event_header.find('span').text.strip()\n",
    "        event_month = event_date.split()[0]\n",
    "        event_day = event_date.split()[1][:-1]\n",
    "        if '-' in event_day:\n",
    "            event_day = event_day.split('-')[0]\n",
    "        event_year = event_date.split()[-1]\n",
    "        \n",
    "        date_string = f\"{event_year}-{event_month}-{event_day}\"\n",
    "        \n",
    "        for row in event.find_all('tr'):\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) == 3:\n",
    "                performance = {\n",
    "                    'Date': date_string,\n",
    "                    'Gender': gender,\n",
    "                    'Event Name': columns[0].text.strip(),\n",
    "                    'Athlete Name': athlete_name,\n",
    "                    'Team': team,\n",
    "                    'Mark': columns[1].find('a').text.strip(),\n",
    "                    'Event Type': 'Individual',\n",
    "                }\n",
    "\n",
    "                performances.append(performance)\n",
    "    \n",
    "    return performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4b00efa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'findAll'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 14\u001b[0m\n\u001b[0;32m      7\u001b[0m naia_teams \u001b[38;5;241m=\u001b[39m get_team_links(naia_response)    \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get the athlete links from the team links\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# ncaa_athletes = get_all_atheltes_links(ncaa_teams)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# naia_atheltes = get_all_atheltes_links(naia_teams)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# TESTING\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m ncaa_athletes \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_atheltes_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.directathletics.com/teams/track/1335.html\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(ncaa_athletes)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[161], line 59\u001b[0m, in \u001b[0;36mget_all_atheltes_links\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m     57\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m     link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcontents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 59\u001b[0m     athlete_links\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_athelte_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m athlete_links\n",
      "Cell \u001b[1;32mIn[161], line 85\u001b[0m, in \u001b[0;36mget_athelte_links\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m     81\u001b[0m athlete_info \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     83\u001b[0m links \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-turbo-frame\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_top\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 85\u001b[0m individual_links \u001b[38;5;241m=\u001b[39m \u001b[43mlinks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindAll\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtn\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(individual_links)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m athlete \u001b[38;5;129;01min\u001b[39;00m individual_links:\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\bs4\\element.py:2433\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   2432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   2434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[0;32m   2435\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'findAll'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "# Get the HTML content of the NCAA and NAIA pages\n",
    "ncaa_response = make_request(ncaa_url)\n",
    "naia_response = make_request(naia_url)\n",
    "\n",
    "# Get the team links from the HTML content\n",
    "ncaa_teams = get_team_links(ncaa_response)\n",
    "naia_teams = get_team_links(naia_response)    \n",
    "\n",
    "# Get the athlete links from the team links\n",
    "# ncaa_athletes = get_all_atheltes_links(ncaa_teams)\n",
    "# naia_atheltes = get_all_atheltes_links(naia_teams)\n",
    "\n",
    "# TESTING\n",
    "ncaa_athletes = get_all_atheltes_links(['https://www.directathletics.com/teams/track/1335.html'])\n",
    "# print(ncaa_athletes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
